{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1\n",
    "Write the log likelihood for a multinomial sample and show equation (4.6).\n",
    "$$\n",
    "\\hat{p}_i=\\frac{\\sum_t x_i^t}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multinomial sample, the outcome of a random event is one of K mutually exclusive and exhaustive states, each of which has a probability of occuring $p_i$ with $\\sum^K_{i=1}p_i$. Let $x_1, x_2,...,x_K$ be indicators where $x_i=1$ if the outcome is state $i$ and 0 otherwise.\n",
    "\n",
    "In one experiment, \n",
    "$$\n",
    "P\\left(x_1, x_2, \\ldots, x_K\\right)=\\prod_{i=1}^K p_i^{x_i}\n",
    "$$\n",
    "We do $N$ such independent experiments with outcomes $\\mathcal{X}=$ $\\left\\{\\boldsymbol{x}^t\\right\\}_{t=1}^N$ where $\\sum_i x_i^t=1$ and\n",
    "$$\n",
    "x_i^t= \\begin{cases}1 & \\text { if experiment } t \\text { chooses state } i \\\\ 0 & \\text { otherwise }\\end{cases}$$\n",
    "The constraint is $\\sum_i p_i=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2\n",
    "Write the code that generates a normal sample with given $\\mu$ and $\\sigma$, and the code that calculates $m$ and $s$ from the sample. Do the same using the Bayesâ€™ estimator assuming a prior distribution for $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q3\n",
    "Assume a linear model and then add $0$-mean Gaussian noise to generate a sample. Divide your sample into two as training and validation sets. Use linear regression using the training half. Compute error on the validation set. Do the same for polynomials of degrees 2 and 3 as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error (linear): 99.62212970325035\n",
      "Validation Error (linear): 101.05000329342418\n",
      "\n",
      "Train Error (polynomial, degree=2): 99.61329168312504\n",
      "Validation Error (polynomial, degree=2): 101.08743320197074\n",
      "\n",
      "Train Error (polynomial, degree=3): 99.61296703776705\n",
      "Validation Error (polynomial, degree=3): 101.08241042597683\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from random import shuffle\n",
    "# generate data with a linear model\n",
    "x = np.arange(0, 1000, 0.1)\n",
    "shuffle(x)\n",
    "y = 4*x + 3\n",
    "# add Gaussian noise, noise~(N(0, 10))\n",
    "y += np.random.normal(0, 10, len(x))\n",
    "\n",
    "# train-validate split, 60% for training, 40% for validating\n",
    "train_x = x[:int(len(x)*0.6)]\n",
    "train_y = y[:int(len(y)*0.6)]\n",
    "valid_x = x[int(len(x)*0.6):]\n",
    "valid_y = y[int(len(y)*0.6):]\n",
    "\n",
    "# train a linear model\n",
    "model = LinearRegression()\n",
    "model.fit(train_x.reshape(-1,1), train_y)\n",
    "error_train_1 = np.mean((model.predict(train_x.reshape(-1,1)) - train_y)**2)\n",
    "print(f'Train Error (linear): {error_train_1}')\n",
    "y_pred = model.predict(valid_x.reshape(-1,1))\n",
    "error_test_1 = np.mean((y_pred - valid_y)**2)\n",
    "print(f'Validation Error (linear): {error_test_1}')\n",
    "\n",
    "# train a polynomial model (degree = 2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "train_x_poly = poly.fit_transform(train_x.reshape(-1,1))\n",
    "valid_x_poly = poly.fit_transform(valid_x.reshape(-1,1))\n",
    "model2 = LinearRegression()\n",
    "model2.fit(train_x_poly, train_y)\n",
    "error_train_2 = np.mean((model2.predict(train_x_poly) - train_y)**2)\n",
    "print(f'\\nTrain Error (polynomial, degree=2): {error_train_2}')\n",
    "y_pred_2 = model2.predict(valid_x_poly)\n",
    "error_test_2 = np.mean((y_pred_2 - valid_y)**2)\n",
    "print(f'Validation Error (polynomial, degree=2): {error_test_2}')\n",
    "\n",
    "# train a polynomial model (degree = 3)\n",
    "poly3 = PolynomialFeatures(degree=3)\n",
    "train_x_poly3 = poly3.fit_transform(train_x.reshape(-1,1))\n",
    "valid_x_poly3 = poly3.fit_transform(valid_x.reshape(-1,1))\n",
    "model3 = LinearRegression()\n",
    "model3.fit(train_x_poly3, train_y)\n",
    "error_train_3 = np.mean((model3.predict(train_x_poly3) - train_y)**2)\n",
    "print(f'\\nTrain Error (polynomial, degree=3): {error_train_3}')\n",
    "y_pred_3 = model3.predict(valid_x_poly3)\n",
    "error_test_3 = np.mean((y_pred_3 - valid_y)**2)\n",
    "print(f'Validation Error (polynomial, degree=3): {error_test_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# train = [error_train_1, error_train_2, error_train_3]\n",
    "# validate = [error_test_1, error_test_2, error_test_3]\n",
    "# sns.barplot(x=['linear', 'degree=2', 'degree=3'], y=train, color='blue')\n",
    "# sns.barplot(x=['linear', 'degree=2', 'degree=3'], y=validate, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q4\n",
    "When the training set is small, the contribution of variance to error may be more than that of bias and in such a case, we may prefer a simple model even though we know that it is too simple for the task. Can you give an example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q5\n",
    "Generate a sample from a multivariate normal density $N(\\mu, \\Sigma)$, calculate $m$ and $S$, and compare them with $\\mu$ and $\\Sigma$. Check how your estimates change as the sample size changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [5.01694065 2.00090362]\n",
      "cov [[ 6.08991548 -2.9627677 ]\n",
      " [-2.9627677   3.44442192]]\n"
     ]
    }
   ],
   "source": [
    "mean = [5, 2]\n",
    "cov = np.array([[6, -3], [-3, 3.5]])\n",
    "# when sample size equals to 10\n",
    "sample = np.random.multivariate_normal(mean, cov, 5000)\n",
    "mean1 = np.mean(sample, axis=0)\n",
    "cov1 = np.cov(sample.T)\n",
    "print('mean', mean1)\n",
    "print('cov', cov1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.4675698 , -3.53264726],\n",
       "       [-3.53264726,  2.81689842]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5684c24e7b5fcef90f2a57823d93e544ff1f9f38d3d0df1544269a25d13d98c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
